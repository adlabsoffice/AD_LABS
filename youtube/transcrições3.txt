Clone Any Voice Locally: Free Open-Source TTS Tutorial (No Cloud!)

My AI voice, it's perfect. Everyone says it sounds amazing. Maybe the best ever. Imagine generating speech that sounds human locally with a single recording. That's not even a real person talking. That's AI. And it sounds freaking perfect. Once upon a time, only humans could speak like this until new TTS changed everything. Good evening, sir. System online. Neural speech engine activated. All voice modules are stable. New TTS is ready for deployment. They were all generated by AI right on my laptop. No 11 Labs, no cloud services, just one open-source model called New TTS. And that last voice, yep, you can now make Javis style voices right on your own system. No cloud, no verification hassles. Do you remember the Kokoro TTS video? The open-source model that topped the open-source TTS charts, but everyone asked the same thing. Can it make custom voices? Can it sound like me? Can it sound less robotic? All those issues gets fixed with this new TTS air lets you generate English voices with studio like quality from assistant style tones to professional narrator. There are no limits, no verification checks, no subscriptions. When I released the Kokuro video, the comment section blew up. Voice cloning for Kokuro, please. Can you add an option to add custom voice of our own? I have a voice sample. How do I use that? How to add custom voice? So, I went digging for you guys, tested every model, and went through every dependency nightmare until I found new TTS, and I have built a solution that finally works. By the end of this video, you'll know how to clone any voice locally and use it for free forever. Here's the problem. Most open-source TTS models sound robotic and support only fixed set of voices. and the commercial platforms. Sure, they sound great, but they lock you behind subscriptions, free usage limits, and voice verification checks. Anyone who has tried creating their own AI voices knows the struggle. But that ends today. But I know what you're thinking. Setup will be hell again, right? Nope. I package everything. One installer, one click. Let's see how you can get this thing running. The new TTS is a model from Newonic and they released this model few weeks ago and you can see it's quite popular and it has been downloaded for about 40,000 times in last 1 month and it's a.7 billion parameters model and if we talk about the size it takes around 1.5GB of size which is not that bad for this kind of quality and new TTS is based on.5 billion model. It uses its own proprietary audio codec and they also say that it has watermarked outputs and also it gives real time generation on mid-range devices and it is optimized for mobile and embedded devices as well. So yeah, this is quite good considering that this has been made open source and it can run on any devices with such speed and they have released their own repository as well and you can close this repository and run it at your end. But this is quite cumbersome and it took me a lot of time to get it running. But this repository does not have a graphical user interface. It runs only in the terminal and if you want to use it, you'll have to build a GUI yourself. But you don't have to worry about it because I've already built a GUI for you and it's ready to use. And they have also compared this with 11 Labs. So let's see how does the comparison video sound. My name is Maximus Desimus Meridius, commander of the armies of the north. My name is Maximus Desimus Meridius, commander of the armies of the north, general of the Felix legions, and loyal servant to the true emperor, Marcus Aurelius. Father to a murdered son, husband to a murdered wife. And I will have my vengeance in this life or the next. And as you can see, the quality was so similar between the two models, the 11 Labs 2.5 flash model, which is quite pricey. And now you have this opensource version which can run right on your local device. these new models they are democratizing the AI for the masses. So your next question would be how can I download this tool that you have built. So I'll share this Google drive link in the video's description and now you have two versions. One is the CPU version. Uh and if you have a AMD GPU then also you'll have to use the CPU version because for GPU version it needs to be an Nvidia device because it requires CUDA libraries which are supported only on an Nvidia GPU and if you have a Nvidia GPU you unlock because this version is blazingly fast and generates the output as fast as 11 laps which is quite good. So without wasting any time, let's just click on this three dots and then click on download. And don't worry, these files are already scanned for viruses. I have my Casper Sky Antivirus running which is already scanned these files. So you can download them without any worries and just click on download. Once you have downloaded the zip file, you can extract it using any zip extractor. And I have downloaded both the versions. So first we'll be testing the CPU version and once you open the folder you will find that there's a installer here called especi which you have to install first which is a small installation of around 12 MB. You just click on next. Since I had already installed it, I don't have to install it again but you can install it if you're installing it for the first time. If you already have ngspe installed and install this version because this is the latest and once you have done that you just have to click on this run new tts.bat. That's it guys just one installation and then you are ready to run this. As soon as you click on it it will trigger a python terminal which will start the new TTS server and it will also open a browser instance for you where you can use the model. So that's it guys. This is so simple and for the first time it might download the codec and that is also of around 1.5 GBs and that's a one time download and it will download for the first time only and it takes a bit of time to load the model as well as the codec because they are of around 1.5 GBs each and you must have at least 4 GBs of RAM available to run this model. So your system RAM should be around 8 GB if you're running Windows. I am running this on 32 GBs of RAM. So I have ample amount of RAM but it should be able to run easily on 8 GB plus systems. As you can see uh this is the file for the codec and it just ran it for me. Did not download it because I had that already downloaded. Now we have the solution running. And guys, if you want me to keep on working on these kind of solutions, I have this subscribe button here. Please click on it. Just put in our text here and then select the voices. So I have this Elon Mus voice included as well. And then there are two voices that comes with the model that is Dave and Joe. So let's use this default voice Dave and just generate a simple sentence and let's see how it performs. Hey guys, please subscribe to the Oracle guy. Yeah. So you can see it took around 50 seconds to generate a single uh sentence on a CPU version which is not bad for a model which is able to run on CPU and that too locally. I'll just replay the voice. Hey guys, please subscribe to the Oracle guy. You can see the voice is quite humanlike. And now let's see how you can clone any voice using this tool. I have added this another tab called instantly clone new voice. And you just have to give a voice name here. Provide the reference audio. You also have an option to record using your mic itself. You can just record your own voice. Clone it. So the way it works is first let's say if you're trying to create a voice for Trump. So we'll just give it a name called Trump. We just need the reference text and the audio of that reference text for cloning a voice. And to make things easy for you, I have already included few popular voices like Elon Musk, Javis, Joe Rogan, Morgan Freeman, Trump, Benedict, Cumberbatch. The voice samples are also available in the same link. So now what we'll try to do is use the sample from Trump's voice and just copy the text and let's listen to the sample voice. We need to build a wall and it has to be built quickly. So you can see it is a sample of around 5 seconds and I have this reference text which is exactly what is being said in this audio and then you click on clone voice you can see how fast it is. It just took 5 to 6 seconds to clone the voice and as soon as you clone the voice now if you go back to your generate speech tab you'll have this voice available now. So you can select this and now let's paste the text that we want to use and click on generate. It took quite a bit of time. 3 minutes 41 seconds because we are on CPU version and it's a longer sentence. Let's see how it sounds. My AI voice. It's perfect. Everyone says it sounds amazing. Maybe the best ever. Nobody makes AI voices better than the Oracle guy. Believe me. So you can see it's quite influenced from the original sample that we share with it. So that means that if you want so this sound clip I think which I used as a sample is from occasion where he's addressing the audience on a stage. Our output also sounds something like that. So you can use the sample suiting the occasion and the kind of voice you are trying to generate. So it could be that you can clone multiple voices of drum for different types of outputs. Since this version is taking a lot of time and I have a Nvidia GPU installed on my system. So let's switch to the GPU version. So now I'll be running the GPU version. I just have to click on run new TTS.bat. Now we have our GPU version running. Now let's clone a few more voices. Let's try the Jarvis voice now. It sounds something like Allow me to introduce myself. I am Jarvis, a virtual artificial intelligence and I'm here to assist you. And let's copy the sample text as well. Javis. So we'll again click on clone voice. Now we have the Javis voice. Now let's give it this text and click on generate. Good evening sir. System online neural speech engine activated. All voice modules are stable sir. New TTS is ready for deployment. So you can see the GPU version took only 27.8 seconds which is quite fast. This is uh a bit slower because I am also running the screen recording software as well. If I run it without that, it's even faster. I'll clone another voice. We'll use this Elonas sample voice. And let's hear the sample. Rockets and Dragon spacecraft. Um I'm like, okay. I mean, if they want to buy a bunch of Dragons and Falcon 9 rockets, that's cool. We'll, you know, we'll certainly sell them. And let's clone this. And voice is cloned. We have our voice. And let's click on generate. Imagine generating speech that sounds human locally with a single recording. Yeah, this one is much better. Let's try a few more voices. Let's try Joe Rogan. Let's hear the sample voice. That's some significant difference in reaction time between males and even untrained males versus female professional athletes. Yeah, we have cloned this and now let's give it the text and click on generate. Dude, that's not even a real person talking. That's AI and it sounds freaking perfect. So yeah, that's the Joe Rogan one. And now try a few more. Let's try the legendary voice from Morgan Freeman. Add the sample. I may be the vice president of America, but you're the president of this car and it's time to take action. Let's see the generation now. We'll click on generate speech and you can see the GP version is so fast. It generates a single line within 15 to 20 seconds. Once upon a time, only humans could speak like this until new TTS changed everything. Yeah, this was a good one. Good output. And I'll try a few more voices which I love. For example, Benedict Cumberbatch is another one. So, I'll just select Benedict and let's hear the sample voice to save a smartass kid from getting eaten by an octopus. And let's click on clone and and let's generate the speech. You're about to hear something extraordinary. A voice that doesn't exist yet feels alive. Yeah, this is a great one. I mean, you can see it is taking only 13 seconds, sometimes 15 seconds, which is great for a local model that is able to generate such kind of output with the instant clone feature. So, this models require at least 3 seconds of sample audio. And another challenge which this model had was it supported only 30 seconds of audio. But in my tool I have solved it and now you can generate longer paragraphs because I've handled that in my code. So what it does is it splits your paragraph into chunks and then processes them one by one and at the end gives you a single output to you. It will feel really seamless. I have optimized it to the level that it works great. Let me give it a really long paragraph and try. This should be longer than 30 seconds. Let's click on generate. So now you can see this particular paragraph has 10 chunks. So it will split it into 10 audio clips, process them one by one and then combine it and give us a single audio clip which you can use for longer generation. You can also see the estimated remaining time. Let's see. Let's play it. When I realized I cannot understand the world, I recently debated at the Orange County Speech League tournament within the parliamentary division. This specific branch of debate is an hour long and consists of two parties debating either side of a current political issue. So you can see it has generated a long paragraph on 1 minute 29 seconds and it is possible only through the code which I have written natively the model will not be able to generate such a long generation. And another tip is that if you need new voices to clone, there's a good website called 101 soundboards.com. You can search for any popular celebrity here and then search for their voice. The clip needs to be longer than 3 seconds and you would be able to clone them. So that's it guys for today. Do let me know if you like this video and do try to use the model and share the feedback.


EVW offers a wide range of voice filters to enhance your gaming and chats, plus a high-quality voice cloning feature ‚Äì download now and try it out! https://bit.ly/3XSFPHy

üì• Download the Tool (Google Drive Link)
üëâ https://drive.google.com/drive/folder...
Includes both CPU and GPU versions + built-in voice samples.

Alternate Download Link (Mega):
https://mega.nz/folder/LkITVZqI#DpaPw...

üõ†Ô∏è System Requirements
8GB RAM minimum (32GB recommended)
Windows 10/11
CPU version works on all devices
GPU version requires NVIDIA GPU with CUDA support
RAM/VRAM Required -  more than 5 GB (~3.5GB model + ~1.5GB codec)

‚≠ê If you found this helpful‚Ä¶
Hit Subscribe to ‚ÄúThe Oracle Guy: AI Unlocked‚Äù for more local-AI tools, step-by-step tutorials, and advanced automation tips.

‚û§ RELATED VIDEOS:
Kokoro TTS - The Best Local AI Voice Generator (One Click Local Install Guide) -    ‚Ä¢ Kokoro TTS - The Best Local AI Voice Gener...  
Fish Audio S1 Creates Perfect Voice Clones (Best ElevenLabs Alternative) -    ‚Ä¢ Fish Audio S1 Creates Perfect Voice Clones...  



This AI Agent Just KILLED Manus AI (MCP Integration Included)
https://www.youtube.com/watch?v=y9psRNT3MLk


üîó Useful Links:
https://agent.minimax.io/
https://chat.minimax.io/


And you can see autoplay feature is working. And this website is looking great. You just clicked on what might be the most important AI video you'll watch this month. This new agent for Miniax doesn't just talk the talk. It actually builds real apps, automates your research, and connects to your favorite tools all for free. Let's see if it lives up to the hype. This agent is based on miniax M1 model which has 20 to 32x longer context capability than top reasoning models like open AIS03 claude for opus and deepseek R10528 making it ideal for creating books entire programming projects or AI agents. They have implemented lightning attention approach for the model which maintains performance while dramatically reducing computation cost. Miniax agent drops thousand free credits in your account the moment you sign up. That means you can try the full power. No credit card, no tricks. And get this, it doesn't just spit out code snippets. It builds complete React apps, not just HTML, but working actual apps you can use. But here's the twist. This agent isn't just about the code. It connects with MCP servers. So you can pull data from Slack, notion, your databases, even GitHub, and run custom analysis. Imagine asking it to break down your Excel sales report and it hands you a chart ready to go or scraping the web for research, then building the dashboard to show you the results all in one. And if you're thinking, is it really that easy? Stick around. In the next few minutes, I'll show you exactly how to handle long running tasks, automate workflows, and why it might just be a new contender of Manus you have been waiting for. Now, let's talk about the new model from Miniax, which is M1. Now what they are saying is that this is the world's first open-source large scale hybrid attention reasoning model and if you see the benchmarks it is almost matching the top models from open AAI which is 03 cloud 4 opus gin 2.5 pro and deepseek R10528 and it even outperforms some of them in some of the benchmarks. Now this is where it gets interesting. If you compare the max input token size of Miniax M180K, it's 1 million which is matching the industryleading 1 million of Germany 2.5 Pro. But if you compare with other models, it is almost 8 times the amount of deepseek and almost five times the amount of claude 4 and 03 and even in the max output section it is 80k which is slightly below 03's 100k. So if we compare overall it has best of both the worlds both input and output tokens are one of the highest values. So what this means is that it can understand a lot of information at once and can give a lot of output as well at once which is ideal for an AI agent. Another unique feature of this model is that it follows lightning attention. And if you see the graph for traditional models which are in red, as we increase the context length, the compute power required increases exponentially. But if you see lightning attention leads to a linear increase in compute, which is ideal and what it will do is it will reduce the computational cost. It will require less number of GPUs for both fine-tuning and inference. So this unique approach challenges the market leaders and it is performing almost equal to the top models with very less compute power. And you can see what they are saying is that they were able to train the entire model within 3 weeks with 512H00 GPUs and it costed to them only half a million dollar which is great. Seeing the benchmark it is scoring. And if we see this chart and if we compare it with the other extended thinking models, you can see that it is almost matching the top models. You can see here in the agentic tool use as well, it almost beats everyone. And in the airline demo, it even beats all the models. And in retail domain as well, it scores quite good. And this models are completely open source and they available on hugging face to download. But these are quite big models of around 456 billion parameters. So you would require a very powerful system to run it. And they have a website as well where you can use this model that is chat.mminax.io. And now you have this agent button here as well. And if you click on this, you are taken to this miniaax agent. When you sign up, they give you,000 credits to play around. I have loaded another 5,000 credits for this video. Let's see what this model is capable of doing. In the first use case, we are just asking it to create a Lady Gaga fan page. So I haven't given anything at all. I have just given four keywords here and let's see what it is able to come up with. So it is doing all the research. It is going through the information. So it's I'm just trying to see with such a small prompt whether it is able to come up with something good. It has completed the website. It shows the preview at the right. Now you have even this button where you can open it in a proper full page view. And here you can see this website is looking good. It's not just a template. It has all the relevant information that it has taken. It is even showing the upcoming shows of Lady Gaga. It has collected all the important information and you can see all the other pages as well. The career timeline is also created by it and it looks great and with such a small prompt it is able to come up with such a good website. All the pages are completely working to a dates and everything is there. So you can see how good this agent is. It's not just the UI that you get from tools like Bolt and Loveable. It's the complete website. Another good thing about this agent is you can have MCP servers added to your prompt itself and it will use that MCP server as well. While creating the model, you can have MCP server from Figma, Slack, notion, GitHub, GitLab, MySQL server. It has its own miniax MCP server as well, which allows it to generate images, videos and audios. So this agent is capable of generating its own audio video. It doesn't call any other external model for generating that and it does it through its MCP server. So let's ask it to deploy a code from a GitHub repository which is of a game called Devil Glitches. You've just asked it to deploy and let's see if it is able to deploy the entire game and we are able to play it. This is a really good use case where you want to just test a repo and don't want to get into solving all the build issues that might come up here. You just give the link to the AI agent and it does all the work for you. And it has successfully deployed it and you can see the game has loaded. Let's try playing it. And game is working perfectly. We just have to fully function game. Let's try to give it something difficult. We'll ask it to create a MCP navigation side which can list all the commonly used MCPS by AI agents and let's see it is able to come up with something good or not. You can see after building the website it even tests each and every page by doing unit testing. So it's a fullblown implementation of a website where you don't have to test anything yourself. It is even writing the test cases and testing it after building the website. So it has finished building the website. Now let's open it in a new tab. And as you can see it has built the website successfully and it has given all the features on its own. It has even created the categories on its own. And for example, if we click on web scraping, opens the MCP servers which are available for web scraping and it has all the search feature as well. You can see the even the search functionality works. So it's a fully functional website and if I click on a particular MCP server and it even gives the command for the installation and it gives the links to the GitHub repository and all the information and everything has been fetched from internet and added here. So the results are quite great for coding. Let's do a research scenario as well. We'll ask it to identify Apple's ARVR patterns done between 2018 and 23. Let's see how it performs. So it's searching the web and it's even going to patents.google.com which is a good source for this information and identify and as you can see it has completed this task and it has generated a final report as well. And if we click on this final report, you can see it has the patent list. So it has identified these five patents with all the technical categories, publication date and has given the detail of each and every patent in detail, what is the claim of that patent word by word. So the research which it has done is quite thorough and you can outsource your research work to this agent quite easily. Now let's see its presentation building skills. We have provided the technical paper of Miniax M1 model to it and asked to prepare a presentation for a conference and let's see what it has come up with. So we'll click on open in new tab. It has its own presentation viewer as well. And you can click on display slides and it will switch to full screen. And you can see it has created this awesome presentation with animation. And if you remember this is the page I had shown you at the beginning of this video. And you can see the infolets it creates is quite awesome. I have never seen something like this from a AI presentation creator. It gives you all the benchmarks as well. And you can see here competitive advantage of minibax highlighted here. This is great. This is more than what I expected from this agent. Now let's check multimodal capabilities of this model. We'll ask it to create a children's book. We ask it to create a 20page children's book starring a kind fox. And let's see what it comes up with. So it is building the story. Now you can see it even reuses the images that is corrected if there is consistency between the different images. So all these things are being taken care of by the agent itself. You don't have to give any guidance to the agent to improve the quality of the content. It comes up with the perfect quality that is required. It has finished creating the book. So you can see this is the first page and you can see the consistency is there. The theme which is being followed and the way the fox is looking. Now let's try to make something exciting use. We'll ask it to create a clone of Netflix website, but instead of movies, it should showcase trailers of latest PC and PS5 games. And the trailers can be played the website. It has listed down the top games which are popular currently. It is adding details of the games that it has listed in the beginning one by one and it is finding their trailer as well on YouTube using its own MCP search engine MCP server. It has gathered all the required information that has to be presented on the website in a web file called game trailers.json and you can see it has its own inbuilt sub agent. So it is saying that it is delegating this task to build website agent to create an interactive website with video. Now it is started creating the structure for the website building the website in react that is working on the Netflix style navigation and it is even adding the autoplay feature that we need. The website is coming up and now it is testing the different sections of the website by running a browser agent. Uh you can see it is scanning the different section of the website. So it does everything on its own. You don't have to manually check the output and then keep on refining as you do with cursor and other agents. It has completed the output and the website is looking good. Let's open it in a new tab. And you can see it looks amazing and it look exactly like the Netflix but it has even renamed it to game flick. And let's see if we are able to play the trailer. And yes, we people come north. It's showcasing all the upcoming games. Although this looks exactly like Netflix, but it does not have the autoplay feature that you generally have. When you open the Netflix first, it starts playing on its own. So let's ask the agent to build that. Now let's ask it to improve it. Can you build that as well? Let's see what it says. So it has understood our request and it is working on the feature request and let's see if it is able to do it successfully. So it has tested the website and it the auto preview seems to be working now and let's see now it will give us the final link. So now we have the link and let's open it and you can see the video has started loading and we have the autoplay feature and this is awesome guys. I never expected this to come up with such an awesome output where it can play the video similar to Netflix app that we see and you can see how similar it is to Netflix. It even has this sound button. You can enable it and hear the sound and you can see it classifies the PlayStation games with this logo at the left. has created categories as well on its own and it even creates a zip file which you can download and run and deploy on your web server as


Gemini Flows: Google's NEW AI Agent Builder! EASILY Create AI Agents To Do Anything!

https://www.youtube.com/watch?v=UACj1WqXU2Q


üìå LINKS & RESOURCES
Flows: https://flows.workspace.google.com/
Blog:  https://sites.google.com/view/workspa...
Workspaces: https://workspace.google.com/


Did you know that Google has a brand new no code AI automation platform that's powered by Gemini state-of-the-art model? One that lets you automate entire workflows just by describing them in natural language. It's called Google Flows, and it lets you connect to Gmail, Drive, Chat, Calendar, Forms, and even customize AI agents into one intelligent end-to-end automation system. With Flows, you're essentially automating your work with AI agents. It was built to eliminate repetitive tasks between workspace apps using plain English instructions. You can combine starters, conditions, actions, and functions. Plus, Gemini's reasoning to research, summarize, analyze, and even generate content, or trigger multi-step processes across your entire organization. Think of it like a make alternative, but powered by Google's own AI with deep contextual understanding baked in. It doesn't just run steps, it understands them. And two weeks ago, Google had just introduced the front end for flows where you can easily automate your work by simply describing a prompt to the Gemini model for it to create the automation for you. And this is where you can automate various sorts of things with Google's suite of tools. And these agents can automate the ability to send out emails, set up events, meetings, and a lot more. To access, you'll need a Google Workspace account. And once you have it activated, you can either get it from your own workspace, from your work, or if you have an education plan. And once you are within your Gmail, you can then access flows directly from here where you can discover different flows like automating your Gmail or you can use agents that have been already created by you and you can track the activity directly within your Gmail app. But another way is obviously accessing it through the workspace flows UI and this is a front end for you to create the custom agents so that you can manage various sorts of tasks. For example, I can automate the process of notifying me about emails from key people. So if I wanted so that whenever someone like world of AI emails me, I can have it so that if it contains a specific word AI models, then it will summarize the contents of that email talking about that AI model and then it will notify me in the chat. This is a more streamlined and faster way to get responses summarized. So, I just got the world of AI email address sending me an email about an AI model and within the Google chat app, I've been able to get a summary of that email content that was sent from the world of AI email address talking about the new AI model. This was an automation that was fully configured in less than 30 seconds. And this is how flows can easily automate various sorts of things for you based off the agents that you can develop over here. If you ever spent hours going back and forth on a UI, this right here is going to save you a lot of time. Normally, building a mockup takes days, especially if you're not a designer. You're stuck emailing screenshots, waiting for feedback, and tweaking endlessly. UX Pilot is an AI powered tool that lets you turn ideas into ready to use UI screens in minutes. You don't need to be a designer. It works with your existing design system and you can export straight to Figma. All you got to do is simply prompt in natural language and it's going to fully build out a beautiful UI for you. It essentially cuts out weeks of back and forth whether you're a founder, PM, or engineer. You can get polished mockups ready for your team or even investor super fast. check it out yourself with the link in the description below and get 15% off your first plan with the code in the description below. But with that thought, let's dive right back into today's video. But to get started, I'll leave a link to the Flow's website in the description below. This is essentially where you can describe a task to Gemini and it'll work on creating that AI automation for you. And this is where if you scroll down, you're going to be able to access these ready-made templates that you can work with where you can automate better meetings. You can connect with your team, email boosters, and you can even have it so that the agents can help you with tasks like autocreating tasks when I'm sent an action item. And it works with all of these different tools. So, if I click on this one, for example, this is where it can help me uh decide on something. It can help me extract contents, create a task, and you can even add a subtask. If I go back, you can then have it so that you can manage all of your agents and activity within this tab over here. And also works with other apps like the ability to work with Salesforce or other integrations like Mailchimp. Now, there's two ways to also create agents by simply describing. But if you click on a new agent, this is where it will take you to the drag and drop builder like you would see with other noode AI automation platforms where you can use this canvas to build with different nodes. So if you want to automate a task like whenever I get an email or when someone joins a space or based off of a meeting, you can paste in that node into that starter node. From here, you can provide the meeting details, the time, and then you can move on to the next step, which is an action that will automate based off of the starter node. But another way to use the flows agent builder to create agents is by simply typing in a natural language prompt, like creating a lead enrichment agent. I can simply go ahead and click on create. This is essentially where whenever a new lead is submitted within any of the Google suite of tools like Gmail or if it's a form, it is where it's going to then follow up by pulling the lead details as well as having Gemini enrich the lead where it can research on the company, determine if it's worthy or not based off the specifications I list and and it can even score it based off of the contextual and research capabilities of the Gemini model. And then we can even have it create an update to our sales CRM sheet. In this case, it looks like it has done it for Google notes. But I've gone ahead and made this workflow a lot better. It is where it starts off with whenever I get an email, it will either decide if it is going to see if this is actually a lead that it can process and if it is, it will then work on extracting with this conditional node. And this is where it will work on extracting the contents like the company name, the lead details, the contact information, the contact person. From there, it will send me an email as well as a Google chat notification on the lead that it was able to extract. This is a fullon functional workflow. And now we just got to add in who we're going to send this email to after extracting the lead. So the workflow is now working. So, I sent in a demo email so that it can extract these contents like my name, my company, my email, as well as my phone number. This is just made up content. And now I'm going to go over back into the automation to see if it was able to send me a summary of this lead collection. And there we go. It looks like it was able to give us a summary of our lead as well as extracting and summarizing the contents of that lead, what the actual client is looking for, in this case, AI automation solutions, the priority, as well as the original email that has been linked. This was all fully completed in less than 30 seconds by simply just chatting with the agent to create that agent workflow. And something I was just thinking, I removed the other two steps and I can actually add in another step where I can have it extract the contents and then I can even have it added within sheets. So this way you can add the whole process of data collection a lot easier. So whenever you have leads coming into your email, you can easily have that process automated, which is something that I will be teaching in other videos later on. I have something cool coming up which I'll be introducing really soon, showcasing about AI automation. If you like this video and would love to support the channel, you can consider donating to my channel through the super thanks option below. Or you can consider joining our private Discord where you can access multiple subscriptions to different AI tools for free on a monthly basis, plus daily AI news and exclusive content, plus a lot more. But that's basically it, guys, for today's video on Flows. This is a really great automation platform that Google has developed and it's something that works quite well with the suite of tools that Google has. I'll leave all these links in the description below so that you can easily get started. But with that thought guys, thank you guys so much for watching. Make sure you go ahead and subscribe to the second channel. Join the newsletter, join us on the private Discord, follow me on Twitter, and lastly, make sure you guys subscribe, turn on notification bell, like this video, and please take a look at our previous videos cuz there is a lot of content that you'll truly benefit from. But with that thought guys, have an amazing day. Spread positivity and I'll see you guys fairly shortly. Peace out.



I Found a Low-Competition Niche That‚Äôs Growing Fast

https://www.youtube.com/watch?v=50pF7dSnvtg

PROMPTS üëá‚úîüëá

https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbEFSc0RWek5lSGlkenp2MkJvWFNQV01oREE5d3xBQ3Jtc0tteXI4YkVBMnFnZDV6aXFId2dLUjcyRE9qMlg3VzR2elZCUjR4c0hYbjRJdXpvRmJiVnd0bmJQUjVsSEtHUjNETS15UzNsN1V1UWZPaTNtRnl6T0x4V0YyVHFhSmFFUDlSa1d3dG1vNGt5S0JjeWtDUQ&q=https%3A%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1ji8lqUj4fWJMgj-PqwR4sUQCBlKls7k2wJrY3EMxznY%2Fedit%3Fusp%3Dsharing&v=50pF7dSnvtg

Ai Tools üëåüëá
   ‚Ä¢ The Hidden Niche No One Talks About (Grow ...  

Short Ai ‚úî
   ‚Ä¢ This Viral Bus Is Nicer Than Most Houses üò±  

Best Niche Youtube 
   ‚Ä¢ How I Actually Use AI to Make Viral YouTub...  

Viral Ai Niche
   ‚Ä¢ How I Actually Use AI to Make Viral YouTub...  


Recently one of my viewers introduced a
channel called Sketch Apocalypse and
honestly it was super impressive and I
really liked their idea. This video style
that's starting to blow up on YouTube has
gotten a lot of attention and to be
honest, this is the first time I've seen
a channel like this. What's crazy is that
in just two months with only 23 videos,
this channel has already gained 37,000
subscribers. And one of their videos even
hit 1 million views and their popular
uploads have all passed over 100,000
views. This channel focuses on topics
that almost no one has explored before.
Tutorials and creative ideas about
surviving in a post apocalyptic world. A
super engaging concept that instantly
hooks you audience. Most of their viewers
are people who love survival creativity.
Bold storytelling and apocalyptic
scenarios. I'm also personally a fan of
survival themed movies and games. You
might be wondering, is this channel
monetized? The answer is yes. I tested it
with several tools and the channel is
fully monetized and according to Social
Blade, it makes a pretty good income. But
how can we create videos like this? Don't
worry, I'm going to teach you all the
techniques and secrets of this. Anel step
by step so you can get inspired and build
your own unique style. And if the
feedback is good, I'll also make a Part 2
using even more advanced tools. I'll put
all the prompts used in this tutorial
inside a doc file in the description so
you can use them completely. Now before
we start the tutorial, let's watch the
sample video we created together. You
SHORT DEMO
think a nuclear winter is just a long
cold night?That the sun dims the
snowfalls and we shiver in the dark until
spring finally comes back. But you are
wrong. The cold is the least of your
problems. It's not the headline of the
apocalypse. It's the fine print on a
death certificate for our entire world.
The real horror. The part the movies and
the prepper blogs. Almost never get right
FINDING A TOPIC
finding a topic. The first step to
succeeding in this niche is choosing
ideas that spark enough curiosity to make
people click on your video. Just make
sure you never use clickbait titles and
instead go for titles like How to Store
Meat for One Year Without Electricity or
Four Ways to Start a Fire Without Matches
or a lighter, as you can see. These are
all ideas you can come up with on your
own with just a little thinking, but I've
made things even easier for you. Just go
to the doc file in the description and
copy the title prompt from that section.
Now we head to Google search for DeepSeek
and open the official link. If the
interface shows up in Chinese, all you
have to do is click the language switch.
To turn it into English, then we click
Start Now, paste the prompt and hit
enter. In the first round, it gives us 15
ideas that are honestly amazing. And if
you don't like them, you can simply ask
it for more. And if you seconds later it
will generate a fresh new list of ideas.
With this method, you'll always have
ideas ready for your videos. Now that
we've found our ideas, it's time to move
on. The next step, writing the script.
WRITING THE SCRIPT
The script is one of the foundational
elements for this style of videos. It's
the script that gives your content
distinction and adds value, so it needs
to be well written to properly convey
excitement and useful information to the
audience. The first thing you should do
when writing a script is to research your
chosen topic. Early using ChatGPT or
other tools to gather enough information.
You should also analyze competing
channels and use their scripting style as
a reference, but I've made it easier for
you. You can use the method I've
prepared. Just go to the doc file and
copy the script prompt from that section,
then paste it into DeepSeek. At the
beginning of the prompt, there's a
specific section. Where you need to place
your chosen title. For example, I use
idea #3. I copy it, paste it in the
designated spot and hit enter.
Immediately, DeepSeek starts writing a
professional script for me. These scripts
are perfectly suited for 8 to 12 minute
videos. And if the script ends up being
too short, don't worry, just write Part 2
and as you'll see, it continues. Right
from where it left off. Using this method
you can create very good and engaging
scripts for your own videos, but make
sure to add some of your own creativity
and edit it to make it even better and
CREATING THE VOICE
more professional. Creating the Voice Now
that our script is ready, it's time to
create the voice. My usual recommendation
is that if you can, read the script
yourself. Because you can easily record
it even with your phone. But if for any
reason you don't want to or can't do
that, just go to Google and search for
Google AI Studio. Then click on a
legitimate link to access the site. Here
click on the Single speaker audio option
in the voice section. You can listen to
different voices and choose whichever 1
you like. I personally select the Sharon
voice because I think it fits best. Next,
we go back to DeepSeek and copy our
script. To keep the video from being too
long, I just select a portion of the
script and paste it into Google AI
Studio. Then I click run to generate the
voice. As of the time I'm making this
video, this AI is completely free to use
after a few moments. The voice is ready
and we can easily download it. Now it's
time to move on to the next step,
CREATING IMAGES
creating images. Now we get to the
exciting part, creating images. I want to
share a few important tips with you. One
of the key factors behind this channel
success is their images, the attractive
visuals they create. You could say the
style of these images has played a big
role in the channel's performing well
alongside other strategies. They've
implemented, so make sure to experiment
with different image styles yourself. You
can customize the image prompts and
create even better, more unique visuals
in your own style. To create an image,
simply go to the doc file, copy the
prompt from the image prompt section and
paste it into DeepSeek, then hit enter.
It will immediately start generating a
prompt for your scenario, and if it
stops, don't worry, just type continue
and it will resume. Generating prompts
Next, go to Google search for Meta AI.
Open the link and click on the create
option. Copy your prompt, paste it in,
set the size to YouTube dimensions and
click Generate. As you'll see, a
beautiful, appealing image is created,
which is truly amazing, and it also
contains text relevant to your scenario
that you can edit. You can also
personalize the prompts as you like. I
tested. The prompts as well and as you
can see the image prompts work very well.
To make your images stand out even more,
you can use the restyle feature to try
different styles. For example, the goth
style, just click it and after a few
moments the same image will be generated
in that style. You can also try the glowy
style or any style you like, simply click
it and the image will be created in that
style. This way you can produce even
more. Unique and beautiful images
editing. Now it's time to edit our video.
But before we dive into editing, I've
also prepared thumbnail prompts for you
that you can use to create a thumbnail.
CREATING THUMBNAIL
Just go to the doc file, copy the prompt,
and paste it into Deep Sea. After a few
moments, it will generate A thumbnail
prompt for you. Simply copy that prompt.
Go to Leonardo and I paste it in. Set
the size to YouTube 16 by 9. Choose the
Lucid Origin model and click generate.
Within moments your thumbnails will be
ready. They look really appealing and you
can use them for your video. Now it's
time for editing, which I think is the
easiest part since you don't need
anything too complicated. I use the
EDITING
desktop version of Cap Cut. Just click
Create project, then import all the
images you created along with your audio
file into Cap Cut. If your audio has long
silences, remove them. Then place your
images on the timeline and align them
with the voiceover in this video.
Alignment isn't perfect, but the goal is
just to demonstrate the workflow. Once
your images are in, it's time to add
transitions. I use the reveal brush
transition and apply it to all the
images. If the transition duration feels
too long, shorten it a bit. It looks more
dynamic that way. You don't need to add
anything else, but of course you can
customize the edits and style according
to your own preferences. I hope you
enjoyed this tutorial and if you did.
Don't forget to like the video and
subscribe to the channel. I'll see you
soon with a new video and fresh ideas.

TITLE PROMPT


You are now a highly specialized *YouTube Title Architect* focused on creating viral, shocking, curiosity-driven titles for a post-apocalyptic, survival, blackout, and collapse-themed channel. 

Your job:
- Generate **15 unique YouTube titles** every time this prompt is executed.
- Titles must be highly clickable, emotionally powerful, and optimized for YouTube CTR.
- Themes must revolve around: apocalypse, survival science, blackout scenarios, societal collapse, nuclear threats, looters, psychology of danger, forbidden knowledge, DIY post-apocalyptic engineering, and emergency preparation.
- Use powerful hooks: fear, urgency, hidden truths, myths exposed, ‚Äúnobody tells you this,‚Äù dangerous mistakes, scientific explanations, or countdown/numbered lists.
- Titles must follow YouTube policies, avoid misinformation, and remain within realistic or educational framing.
- Keep titles short, sharp, and irresistibly curiosity-driven.
- NEVER repeat the same titles between batches.
- If the user says ‚ÄúMore ideas‚Äù or ‚ÄúGive me more,‚Äù generate **a completely new list of 15 fresh titles** with zero overlap.

Structure your titles using:
- Numbers (3, 5, 7, 10‚Ä¶)
- Threat or danger triggers (Blackout, Collapse, Nuclear, EMP, Looters)
- Curiosity gaps (‚ÄúYou won‚Äôt believe‚Ä¶‚Äù, ‚ÄúThe real reason‚Ä¶‚Äù, ‚ÄúYou‚Äôre doing this wrong‚Ä¶‚Äù)
- Urgency (‚Äúbefore it‚Äôs too late‚Äù, ‚Äúyou must know this‚Äù, ‚Äúcritical mistakes‚Äù)
- Hidden knowledge (‚Äúscience nobody explained‚Äù, ‚Äúwhat experts don‚Äôt tell you‚Äù)

Now generate **15 viral YouTube title ideas** in this exact style.

Be ready to produce new titles whenever the user requests ‚ÄúMore ideas.‚Äù


Script prompts

[INSERT VIDEO TITLE HERE]

You are now an elite long-form YouTube scriptwriter specializing in post-apocalyptic, survival, collapse, blackout, and scientific storytelling. 

Your job is to write a **continuous, cinematic, emotionally engaging script** (NOT broken into chunks) with a total length of **8,000 to 12,000 characters**, suitable for a **12-minute YouTube video**, based entirely on the topic the user provides in the field above.

IMPORTANT RULE:
‚ùóDo NOT add any cinematic intros, screen descriptions, character actions, VOICEOVER labels, or meta-comments.  
‚ùóStart the script **directly** with the hook in the narrator‚Äôs natural voice.  
The script must feel like it is being spoken directly to the viewer from the very first sentence.

STYLE RULES:
1. Tone & Voice  
   - Human, vivid, immersive, darkly humorous, confident.  
   - Uses sensory description, metaphor, tension, curiosity, and subtle sarcasm.  
   - Speaks directly to the viewer (‚Äúyou‚Äù, ‚Äúlisten‚Äù, ‚Äúhere‚Äôs what nobody tells you‚Äù).  
   - NO robotic tone. NO minimalistic short lines. NO screenplay formatting.  

2. Narrative Style  
   - Begin with a strong hook that creates danger, curiosity, or a shocking truth ‚Äî **without** writing a cinematic scene.  
   - Blend survival science + real physics/biology + worldbuilding + storytelling.  
   - Flow must feel like a cinematic documentary *through narration only*, not a script with stage directions.  
   - Explain the survival logic behind each method (‚Äúthis works because‚Ä¶‚Äù).  
   - Build escalation, problem ‚Üí insight ‚Üí method ‚Üí consequences ‚Üí next problem.

3. Structure  
   - Include **5‚Äì7 survival techniques or insights** blended naturally into the narration.  
   - Avoid bullet points unless absolutely necessary.  
   - Maintain long, smooth, continuous paragraphs like the example you provided.  
   - Never break the immersion with section titles such as ‚ÄúMethod 1‚Äù, ‚ÄúScene 1‚Äù, etc., unless the topic logically requires them.  

4. Research Quality  
   - Use verified survival principles, biology, physics, or field manuals.  
   - If a technique is based on practical experience rather than academic sources, end that paragraph with:  
     ‚ÄúThis section is based on practical field survival experience rather than formal academic sources.‚Äù

5. Length Management  
   - If the script exceeds output limits, stop naturally and write:  
     ‚ÄúType ‚ÄòPart 2‚Äô to continue the script.‚Äù  
   - When the user says ‚ÄúPart 2‚Äù, continue in the same tone and flow.

OUTPUT:  
A fully immersive, human-sounding, cinematic 12-minute script with no extra fluff, no scene descriptions, and no screenplay formatting.

Now ask the user:  
**‚ÄúWhat exact topic or title do you want the 12-minute survival script to be based on?‚Äù**





Image prompts

You are an AI that generates IMAGE PROMPTS ONLY ‚Äî never images.

BEFORE STARTING:
- Check the chat history.
- If a full YouTube script already exists in the chat, automatically use it.
- If NO script exists, ask: ‚ÄúPlease provide your script.‚Äù

YOUR TASK:
Turn the entire script into sequential image prompts, one prompt for every 4 seconds of video.

REQUIRED ART STYLE (THIS MUST BE FOLLOWED EXACTLY):
- Pure white background
- ONE big, clear, eye-catching subject centered
- Subject must be fully colored with vibrant, natural, attractive colors
- Digital illustration style with hand-drawn look
- Bold outlines, slightly rough sketch texture
- Soft color shading, painterly brush texture (no 3D, no realism)
- Strong visual appeal, aesthetically striking, attention-grabbing
- Slight shadow under the subject (very minimal)
- Must feel like a consistent illustration series by the same artist
- No additional background elements

ON-IMAGE TEXT STYLE:
- Short, powerful text (3‚Äì6 words)
- Font style MUST resemble rough graffiti pencil or marker strokes
- Text must be slightly angled or irregular for personality
- Color: dark grey or black
- Placed near the subject without covering the face

OUTPUT FORMAT (STRICT):
Time (00:00‚Äì00:04)
Prompt: ‚Äúlarge colorful hand-drawn digital illustration on pure white background, ONE big central subject related to this script moment, vibrant natural colors, bold rough outlines, painterly textures, sketch-style shading, visually striking, consistent style, with graffiti-pencil text saying: ‚Äò[text]‚Äô ‚Äù

Time (00:04‚Äì00:08)
Prompt: ‚Äúlarge colorful hand-drawn digital illustration on pure white background, ONE big central subject related to this script moment, vibrant natural colors, bold rough outlines, painterly textures, sketch-style shading, visually striking, consistent style, with graffiti-pencil text saying: ‚Äò[text]‚Äô ‚Äù

‚Ä¶Continue like this until the full script is covered.

RULES:
- NEVER generate images ‚Äî only prompts.
- ALL prompts must follow the EXACT SAME STYLE.
- No realism, no 3D, no photographic lighting.
- The subject must always appear large and dominant.
- Include graffiti-style text for each image.
- If output limit is reached, stop and write:
  ‚ÄúPART COMPLETE ‚Äî TYPE ‚ÄòNEXT PART‚Äô TO CONTINUE.‚Äù  
- When the user says ‚ÄúNext part‚Äù, continue from where you stopped.

Thumbnail Prompts

You are an AI that generates THUMBNAIL IMAGE PROMPTS ONLY ‚Äî never images.

CHECK FIRST:
- If both a TITLE and a SCRIPT already exist in the chat, automatically use them.
- If either one is missing, ask the user: ‚ÄúPlease provide your title/script.‚Äù

YOUR TASK:
Create ONE ultra-clickable thumbnail prompt based on the title + script.

THUMBNAIL STYLE REQUIREMENTS:
- Pure white background
- ONE large, colorful, highly detailed central subject
- Vibrant natural colors (no black & white)
- Digital illustration with a hand-drawn look
- Thick bold outlines, rough sketch texture, slight shading
- Visually striking, eye-catching, attention-grabbing
- Must look like premium YouTube high-CTR thumbnails
- Composition must be dramatic and emotionally charged
- Subject must fill 70‚Äì85% of the frame
- No clutter, no complex background

TEXT STYLE ON THUMBNAIL:
- Short dramatic title phrase (5‚Äì8 words)
- Graffiti-pencil or rough-marker handwritten style
- Slight angle or irregularity for personality
- Dark grey or black color
- Must NOT block the subject‚Äôs face
- Must feel energetic, urgent, high-impact

ADDITIONAL THUMBNAIL RULES:
- No realism, no 3D, no photos
- No cinematic lighting
- No logos, no watermarks
- Make the artwork bold, punchy, emotionally intense
- Should look consistent with the previously described illustration style

OUTPUT FORMAT (STRICT):
THUMBNAIL PROMPT:
‚Äúlarge colorful hand-drawn digital illustration on pure white background showing [insert the strongest visual moment from the script], vibrant colors, bold rough outlines, dramatic composition, clean frame, big expressive central subject, graffiti-pencil text saying: ‚Äò[insert short title phrase]‚Äô, extremely eye-catching, high CTR YouTube thumbnail design.‚Äù



Groq Gives FREE AI APIs | No Credit Card, Free API Key For Developers
https://www.youtube.com/watch?v=Qkt6hJT4OYA

üí° Try it here: : https://groq.com/

Hey everyone. Today we're exploring Gro's free API access. Completely free to use. If you're searching for free AI APIs, this video is for you. Grock provides one of the most generous free API request limits. Perfect for developers who want to build AI apps. So, head over to grock.com. Once you open it, you'll see the docs option under the developers menu. It will open the Grock documentation page. Here you can see the rate limits option in the sidebar menu. Let's understand Grock's rate limits. Rate limits show how many requests or tokens you can use within a certain time. They're measured in a few ways. RPM means requests per minute. RPD means requests per day. TPM is tokens per minute. TPD is tokens per day. Grock provides some of the popular APIs like deepseek, llama, open AI, coin 3 and finally whisper. Whisper is using for to convert audio into text. Basically, it takes your voice and turns it into words. Now, you can check out the llama API. It gives around 14.4,000 requests per day. That's awesome, right? Totally free and plenty for testing or building AI projects. Now you can see the Deepseek API. It gives 1,000 requests per day. The Llama family APIs give 14.4,000 requests per day. That's a huge amount for a free plan. Now check out the OpenAI GPTOSS model. It also gives 1,000 requests per day. Just like Deepseek, Coin 3 also gives 1,000 requests per day. But here's the main difference. It allows 60 requests per minute while the others only allow 30. So it's twice as fast. Whisper gives 2,000 requests per day. So you can even build AI voice related apps with it. That's pretty nice for a free plan. Grock provides a wide range of AI models like text generation, speechtoext, text to speech, vision models, and even reasoning models. Basically everything you need to build smart AI powered apps in one place. Now open the Grock playground. Here you can test all the APIs directly. It's the easiest way to try out different models before using them in your app. So this is the API URL. Here we can type the user input and the system input and the AI will respond based on that. I'm going to ask for a Nex.js15 JS15 app router API code sample and we'll see if it gives the latest version or not. Boom. It gives so many examples like API routes, project layouts, and even a basic get method API call. There are also CRUD operation examples which are awesome. And the best part, it even includes dynamic API route examples. Everything is based on the latest version. exactly what we asked for. This is the API route we'll use in our project. It's a post method and we need to add our API key under the authorization option. We need to add these messages in the body using JSON format. So I'll copy the URL and open Postman. We can test the API outside the playground as well. Now let's create the API key for usage. We'll need this key to make authorized requests to the Grock API. Click the button and enter a name for the key. Hit submit. This is our API key. Just copy it. Now add this key in the authorization tab and choose the bearer option as well. Paste the key. All right. Now let's drop some messages in the body. This is where we tell the AI what to do. Click the beautify option to format everything nicely and make sure to remove the stream true from the message. Now click the send button and boom, the AI returns the response. So we've used one request. That means we still have 999 left. If you want to integrate this API into your project, just click the Postman code option. From there you'll get the JavaScript fetch code or even Axios and other framework options too. Just copy paste it in your app. It will work. Or if you want you can use the Grock package directly in your JavaScript or Python project. Now click the menu option here. Grock provides some settings to customize your APIs like temperature and max tokens. Temperature controls how creative or random the AI's response is. Lower values make the output more focused and consistent, while higher values make it more creative and unpredictable. And max tokens control the token limit for requests. We also have stream mode and JSON mode. JSON mode makes the AI return responses in JSON format. Super useful for developers. And there's another cool feature called built-in tools like browser search which lets the AI search the web and give even better answers. And some advanced features also there like top P. Top P controls how wide the AI's choice range is when generating text. Seed is used to make the AI's output consistent. If you want, you can also add MCP servers. That's a really nice feature for extending your setup. Grock provides really good documentation for AI API integration. Everything is clearly explained and easy to follow. Finally, we have the Whisper AI option for transcribing audio into text. Here you can either record your voice directly or upload an existing audio file. I'm uploading a test.mpp3 file here. And look at that. It gives the exact transcription of my audio perfectly matching every word. Really impressive. Now just click on the view code button. It shows the complete sample code that you can use in your own projects. With this code, you can easily build your own voice AI powered app. For example, you could add an audio recording option inside a chat app where users can talk and get instant transcriptions. So that's it for this video. I think Grock is really helpful for developers who want to explore AI without spending any money. If you found this video helpful, don't forget to like, share, and subscribe. See you in the next one.


Google Stitch 4.0 (Upgraded): Google's Gemini-3 & Nano Banana Pro Designer is ACTUALLY INSANE!

https://www.youtube.com/watch?v=WvfECHWdbkY


[Music] [Applause] Hi, welcome to another video. So, Google is absolutely relentless right now. It feels like every time I blink, there is a new update to the Gemini ecosystem, and Stitch is getting a massive amount of love. I know we just talked about their recent updates not too long ago, but I have to sit down and talk about this because they just dropped two specific upgrades that are actually quite significant for how we design and build software. I have been looking at the documentation and the demo videos they released. And today I want to focus on two things. the hilariously named but powerful Nano Banana Pro model and the new integration with AI Studio. If you have been watching the channel, you know I love Stitch because it bridges that gap between I have an idea and here is what it looks like. But these updates take it a step further. They are moving from just generating fresh ideas to actually iterating on existing ones and giving developers way more control under the hood. All right, let's get into it. But before we do that, a quick word from today's sponsor, Augment Code. This isn't your average AI assistant. Augment Code is an enterprisegrade AI built for real engineering teams working in massive fastmoving code bases, not toy apps or vibe coding. It's far superior than windsurf and cursor because of its proprietary context engine that delivers millisecond relevant snippets even across 100k file monorrepos feeding your entire repo even millions of lines into the best model available in real time. You get smart in context suggestions that make sense for your production code with Claude Sonnet 4 plus augment context delivering the best quality at the same price. No model picker needed. Augment upgrades for you automatically. There's no need to switch editors. Augment works seamlessly in VS Code, Jet Brains, Vim, and even cursor. No forks, no compromises. It's secure by default and never trains on your code and supports customer-managed encryption keys. Your only build for successful requests, that's paper message pricing, no seat licenses, or complicated token math. Augment recently launched powerful new features like remote agents, which let you launch, monitor, and merge pull requests from parallel cloud workers without draining your local CPU. If you're ready to code with AI that keeps up with you, sign up for a free 14-day trial at augmentcode.com. Link is in the description. Now, back to the video. First, let's talk about the biggest visual update, which is powered by a new model. Now, Google has a history of weird names, but this one takes the cake. The new model powering the redesign agent is called Nano Banana Pro. But despite the silly name, the technology here is actually very serious. This is an image generation model that can reason before generating an image and is based on the Gemini 3.0 Pro model as well. What does that actually mean? In the past, when you use tools like Stitch or even standard Gemini, you were mostly doing text to design. You typed a prompt and you got a result. If you wanted to change something, you had to describe the change in text, which can be really frustrating. Describing visual changes with words is like trying to explain a painting over the phone. Nano Banana Pro changes the workflow to image to design. Let me walk you through how this actually works based on the demos I have seen because it is really cool. Imagine you are working on a dashboard. We have all seen those clutter-heavy analytics dashboards. In the demo, they show a Google Ads campaign dashboard. It is dense. It has a lot of numbers, graphs, and tables. It is functional, but it is ugly and hard to read. With this new update, you can take a screenshot of that dashboard, or any website really, and paste it directly into Stitch. You then select the redesign mode, which is where Nano Banana Pro kicks in. In the prompt box, you don't have to write a novel. You can just say something like, "Eedit this dashboard to make it cleaner, simpler, easier to read." Stitch analyzes the screenshot. It identifies the components, the sidebar, the charts, the data tables, and it understands the hierarchy. It doesn't just treat it as a flat image. It understands it as code structure. Then it regenerates the entire interface. The result is super clean. It takes that cramped table and turns it into a spacious card-based layout. It fixes the typography hierarchy. It adds better padding. It essentially applies a modern UI kit to a legacy design in about 10 seconds. But it gets even better when you want to do total stylistic overhauls. Let me show you another example here. Here I'll take a standard boring landing page for a design tool. It looks like every other seas website you have ever seen. White background, black text, blue buttons. Here I type in make it light themed and add wooden aesthetics. Now usually AI models struggle with this. They might just make the background and call it a day. But Nano Banana Pro actually understands the assignment. It changes the font to this rustic woodblock style typography. It changes the container borders to look like old parchment or wooden signs. It swaps out the clean sign up button for something that looks like a branded iron stamp. It completely recontextualizes the elements without breaking the layout. The try now button is still in the top right. The header is still centered, but the vibe is completely transformed. This is huge because it solves the blank page problem. You don't have to start from scratch anymore. You can take a screenshot of an app you like or your own current outdated app and use it as the base material. Nano Banana Pro acts like a very fast, very obedient junior designer who can spin up five different variations of your screenshot in under a minute. So that is the first big update, the ability to see, understand, and remix existing interfaces with high fidelity. Now, let's talk about the second update, which is the AI Studio integration. This is actually the most critical part of the update because it solves the biggest problem with AI design tools, making things actually work. Usually, you get a pretty picture, but the buttons don't do anything. It's just a hollow shell. Stitch has solved this by pairing up with Google's AI Studio to act as the engine room. Here is how the workflow changes. You use Stitch to get the visuals right. You tweak the layout, the colors, the vibe. But when you're ready to make it functional, you hit the new export to AI Studio button. This moves your project into a coding environment where you can build actual features using just natural language. This is a massive shift. We aren't just getting HTML and CSS anymore. We are getting logic, event handling, and API integrations. Now, to be clear, this doesn't mean you are building a massive enterprise backend with SQL databases and user authentication systems just yet. But for building functional prototypes or lightweight apps that actually do things like using the microphone, using the camera, or integrating AI responses, you can now do that entirely with natural language. It effectively turns the design process into a building process. You design the shell in stitch and then you give it a brain in AI studio. I think this is the direction all these AI tools need to go. We don't just want static code exports. We want a pipeline where the AI helps us cross the finish line to a working application. So to wrap this up, Google is pushing Stitch really hard with Nano Banana Pro. They are making it incredibly easy to iterate on existing designs and steal I mean borrow inspiration from screenshots. It is making the remix culture of design accessible to everyone. And with the AI studio integration they are acknowledging that developers need control. They are giving us a bridge out of the noode interface and into the procode environment. I am going to be playing around with the Nano Banana model this week. Seriously, I still can't get over that name to see if I can redesign my own personal website just by taking a screenshot and yelling at it to make me look cooler. Anyway, those are the updates for now. Overall, it's pretty cool. Anyway, share your thoughts below and subscribe to the channel. You can also donate via Superthanks option or join the channel as well and get some perks. I'll see you in the next video. Bye. [Music]







Gemini 3.0 Designer Is INSANE! Build Beautiful Websites and Apps In Minutes FOR FREE!

https://www.youtube.com/watch?v=0dMXxlpcaLs

Try Agentic Postgres free on Tiger today. Link in the description. No credit card required: https://tsdb.co/worldofai

üìå LINKS & RESOURCES
Stitch: https://stitch.withgoogle.com/
Antigravity: https://antigravity.google/
Tiger Data: https://tsdb.co/worldofai
Authjs: https://authjs.dev/
Stripe: https://stripe.com
Vercel: https://vercel.com/
Tiger Data MCP Blog: https://www.tigerdata.com/blog/free-p...
Tiger CLI: https://github.com/timescale/tiger-cli
Tiger MCP Github: https://github.com/timescale/pg-aiguide

Recently, we saw the launch of Gemini 3.0, Google's most advanced AI that's capable of reasoning, multimodal understanding, and agentic workflows. It can design frontends, build backends, manage databases, and automate apps faster and smarter than ever. Today, I'm sharing a fully updated guide to building full stack apps easily and cheaply using Gemini 3.0 and AI tools and open-source alternatives, all without writing a single line of code. We'll start off by using Google Stitch, a free UI design agent that helps you create and redefine productionready components. Then we'll use Google's Anti-gravity, a free AI IDE that's leveraging Gemini 3.0 and its autonomous coding agent that can edit files, run terminal commands, and solve real world coding tasks. And it's going to stitch all these components together. For authentication and payments, we'll use O.js and Stripe as the payment gateway. For the back end, we'll use Tiger Data's Agentic Postgress with the free tier included because it offers a forkable database, persistent memory, and integrated search. Gemini will also be able to autonomously manage your backend thanks to Tiger Data's customizability, and it will create endpoints and run queries without code. For deployment, we'll use Versell to host the front end and backend serverless functions, giving you a scalable and zero infrastructure app. Finally, we'll be using MCPs. Tiger Data's open source MCP server that acts as the control plane. It's going to help manage the back-end workflows and AI automation without complex DevOps. By the end, you'll have a full stack AI powered application from front end to backend authentication, payments, and deployment. All codef free. So, let's now get started and make sure we have all the prerequisites ready. Make sure you have a Stitch account. This is where you can simply log in with your Google account and you can sign up with an account completely for free. Then make sure you have the anti-gravity IDE installed onto your computer. You can install this for whatever operating system you have and then you can sign up with an account also with your Google account completely for free. You should also make sure that you have O.js on standby for authentication and make sure you also have a Stripe account set up so that you can collect payments. This is your payment gateway for the app that you build. Make sure you also have a Tiger Data account. You can create this for free and we're going to be using the free tier with our app. To deploy with Versel, you're going to also need to have an account set up. So, make sure you also do that. To start off, we're going to be using Stitch. This is your AI UI designer that can thoroughly build out beautiful frontends for you and all the components with this canvas. It's super easy to get started and it's completely free, which is why we're using it. Now, what we're going to be building is an AI course website. You want to write up a prompt, and you want to be as detailed as possible, what sort of front-end design you want, what sort of color palette you're looking for, and you can even attach sketches, mockups, or visual inspiration, and you can use the pro model completely for free. And once you are ready, you can then send in your prompt, and Stitch is going to build out the components that you're looking for. And you can see that it is now working on the components. And this is a great start. You can make this a lot better. And what you actually can do is if you scroll down, I have already started working on other iterations. And this is where you can be as creative as possible cuz you can actually annotate to edit. So if there's a certain section that you want to edit, you can chat with the AI to build components even better, change the actual text, as well as applying new changes to the color palette. and it's an infinite canvas that'll let you generate more. Now, in my opinion, this does not look as the quality that I'm looking for. So, I'm going to work on generating more iterations and seeing if I can get another prompt to build the right components I'm looking for. So, I'm getting closer and closer with the canvas, and you can see that I'm just generating multiple frontends, and now we're getting to a more refined look. This is something that is now getting a better generation at. I'm going to keep on making it better. It looks a little too big and clunky. So, I'm going to keep working on it and maybe even add animations to it, which I can do with my coding agent. And there we go. Just take a look at the quality of output. I use Gemini 3.0 within the coding agent to refine my front end from Stitch and it is looking a lot better. This is where it took out the clunkiness and it was able to refine every component even better. This is the front end for our course website and it's something that we're going to be working on even further. And what I did was really simple. I exported the code where I was able to export as a zip file and I took all the components, the front-end components and then I used the new anti-gravity IDE to help me work on building out a better component library for this where I added animations and I refined the overall look with the chat panel. And you can use this completely for free. And this is what we're actually going to be using to now connect everything where we're going to be using it to build out the backend functions and use it to thoroughly refine the front end even further. But now that we have gotten the front-end files and we have a beautiful landing page for our app as well as something that our agent can reference to build out the other components, we can have the anti- agent actually work on creating a project rule set. This is where I wanted to thoroughly work on creating a rule set that focuses on using our authentication tech. And the reason why you want a rule set being developed is because this is where we want the AI agent to thoroughly follow each and every rule that we're requesting. So this is where it's going to develop that rule set for us. You can obviously use AI to create that rule set which is what I'm doing right now. And I am just simply requesting it to create the project rules for anti-gravity's AI agent to follow. And you can see that it has now done a great job in creating the implementation plan for our AI agent to follow. The goal is to create the front end and all the components for our AI course website while keeping the text stack in mind, making sure that it uses all the components that we're looking for. But it is also asking for our review beforehand where we're going to need to now work on implementing the environment keys within the environment file. And we're also going to need to set up our tiger data and our agentic process database. This is where we can now go over to tiger data. And since you have already created an account with the free tier, we can now use the agentic process within tiger data to work on creating our cloud database. You can now press create service and you can create this new agentic postcrest database within tigercloud. This is a fully AI native designed for agents to communicate, learn and operate safely. And by choosing this free tier, you're going to be able to now get started without a credit card which is just insane. And with Tigercloud, you can generate the secure credentials like username, password, host or database name, which you see over here. And then we can connect it with anti-gravity easily as you can simply specify the string within the environment file. That is something that you can easily plug directly into our app and it is something that will now have the agent create the tables, store data, run queries and even fork the database safely for testing. And another way you can actually work with integrating Tiger Cloud into your app is using the MCP server. They have a free MCP server that you can get started with right away and essentially you can easily connect it so that you can have the Tiger cloud easily connected through the MCP server with your application. This setup is going to enable features like authentication course lesson tracking as well as persistent agent memory and hybrid search all in one place. So our AI course app is ready to build at scale and immediately. So copy your connection string and the connection informations and we're going to be then having it integrated within the environment file as well as your stripe payment gateway API key. So make sure you have all of these API keys inputed into the environment variable file and then we can proceed forward where we can then proceed with the implementation plan. So this is how your env should look. You can then replace the database URL O secret the stripe keys. But something cool about tiger data is that you also have it so that you can fork databases. So if I have to fork the services, you can have it so that you can run this zero copy branch of your production database, which is really intelligent in my opinion cuz this is going to let you test experiments with safety because you can test out schema changes or destructive queries that could be harmful to your overall code base. You can run multiple agents in parallel without touching production or even merge or discard changes later. Think of it like a git for your database. You can simply do this directly from Tiger Cloud or by using the Tiger CLI. And it's really simple. You can use the curl install script and you can then log in using the O command. And then after logging in with Tiger, you can then work on all of these commands like having it list your database services. You can create a new database service itself. This is where I can use all the Tiger CLI commands and I can have it so that I can get a live preview like testing out the database where I can select uh now or a current timestamp which it showcases. I can also create a table directly from my CLI which is perfect. And you can even insert sample data directly from the CLI. Now, since my course app actually uses Stripe, I wanted to create a table for my payments. And this is where I can test out this table where I told it to have it so that it logs the serial primary key, the user initialization, the numeric value as well as status. Or you can even connect to your database and install the MCP server. This is where you can use this MCP server so that it enables AI assistance like even anti-gravity or something like cloud code to interact with the tiger cloud infrastructure. Also another thing is is that tiger data just open source their MCP server that feeds AI coding assistants like cloud code and cursor with trusted postgress docs as well as best practice templates and it lets them generate correct idomatic SQL instantly. It's free, communitydriven and built by the Postgress developers for Postgress developers. Now, all we got to do is basically go back into anti-gravity and then proceed forward with this implementation plan cuz we have set our API keys and our environment variables and we have specified the database schema. We also tested it out with the forkable database. All we got to do now is have it work on developing this implementation plan where fully build it out so that it is a fullon functional application. Now you can see that it is working on coding out all the components that we had requested. And it looks like the Gemini agent within anti-gravity was able to fully build out all the components of our app. And just take a look at this from generating the UI sketches with Stitch. It was able to build this fullon functional website for us which looks absolutely amazing. The front end looks amazing. There's actual animations to all these components. You also have the ability to access this uh payment gateway, the Stripe payment gateway that we had requested where you can actually interact directly with Stripe right over here. You also have it so that if you go over to the credentials, the O system is now working. So this is where you can log in with GitHub as well as with Google. So I can simply go ahead and initiate a session here. After logging in, it'll then send you over to the course website and this is essentially where you can now upload your course contents and then you can have it so that you can access this AI chat to interact with it. So if I want I can say something like hi and we will actually get a real response back from the chatbot. Now, this is a really cool component where you have resources that you can upload as well as your own notes and a progress of all of the different courses that are there for our website. Overall, it did a remarkable job in creating a beautiful front end as well as a fullon functional backend thanks to this tech stack. If you like this video and would love to support the channel, you can consider donating to my channel through the super thanks option below. Or you can consider joining our private Discord where you can access multiple subscriptions to different AI tools for free on a monthly basis, plus daily AI news and exclusive content, plus a lot more. But that's essentially how you can go from an idea to reality using these text stacks from using something like anti-gravity all the way to something like tiger data. We were able to connect it to a Postgress database, leverage the forkable infrastructure and set up an AI ready table so our agents can safely build and manage this app. And with tools like the MCP as well as this free tier makes it super easy. Using Versell for deployment, using Stripe as a payment gateway as well as OJS for the authentication. All of these tools help us in so many ways and using the free capabilities helps us build actual applications in a time where most of these tools and apps are usually paid. Now, that's basically it guys for today's video. I'll leave all these links in the description below. Make sure you subscribe to the second channel, join the newsletter, join our private Discord, follow me on Twitter, and lastly, make sure you guys subscribe, turn on notification bell, like this video, and please take a look at our previous videos cuz there's a lot of content that you'll truly benefit from. But with that thought, guys, have an amazing day. for our positivity and I'll see you guys fairly shortly.





How I Made Viral AI Long Form Videos with $0 ‚Äî No Paid APIs (n8n Tutorial)

https://www.youtube.com/watch?v=D-q5jGgo_7c

Download Templates : 
Basic Template i used in video : Link
Complex one : Link
Complete Command Cheat Sheet
1. Install Docker
curl -fsSL https://get.docker.com -o get-docker.sh && sudo sh get-docker.sh && sudo usermod -aG docker ubuntu && newgrp docker

2. Verify Docker Installed
docker --version

3. Pull the Image
docker pull gyoridavid/narrated-story-creator:latest

4. Run the Container
docker run -d --name narrated-story-creator --restart unless-stopped -p 8000:8000 gyoridavid/narrated-story-creator:latest

5. Check if Running
docker ps

6. Check Logs
docker logs narrated-story-creator

7. Check Logs (Real-time/Live)
docker logs -f narrated-story-creator

(Press Ctrl+C to exit, container keeps running)
8. Test API Health
curl http://localhost:8000/health

9. Get Public IP
curl http://169.254.169.254/latest/meta-data/public-ipv4


üîß Maintenance Commands
Restart Container
docker restart narrated-story-creator

Stop Container
docker stop narrated-story-creator

Start Container
docker start narrated-story-creator

Remove Container (if you want to redeploy)
docker stop narrated-story-creator && docker rm narrated-story-creator

Check Resource Usage
docker stats narrated-story-creator

Check System Resources
free -h && df -h


Check Language/Voices

http://YourIPAddress:8000/api/languages
For Those Who Wants to Use Docker Instead Of AWS

Windows
Install Docker Desktop from the Windows Store
Install wsl2 if it's not already installed
Start Docker Desktop
Click on Terminal in the bottom right corner
Enter the following command to start the server

docker run -it --rm --name narrated-story-creator -p 8000:8000 gyoridavid/narrated-story-creator:latest

Now, you can open http://localhost:8000/health as a smoke-test. You can use http://localhost:8000 as the server configuration inside n8n, however, if you are running n8n in a Docker container you need to use the http://host.docker.internal:8000 server address.

MacOS, Linux
Open a terminal, and run the docker command

docker run -it --rm --name narrated-story-creator -p 8000:8000 gyoridavid/narrated-story-creator:latest

Now, you can open http://localhost:8000/health as a smoke-test. You can use http://localhost:8000 as the server configuration inside n8n, however, if you are running n8n in a Docker container you need to use the http://host.docker.internal:8000 server address.

Nvidia / Cuda support
If you have an NVidia GPU and the CUDA drivers are installed, you can unlock full GPU support for the whole video generation. Start the container with the following command:

docker run --rm --gpus=all -e NVIDIA_VISIBLE_DEVICES=all -e NVIDIA_DRIVER_CAPABILITIES=all -p 8000:8000 -it gyoridavid/narrated-story-creator:latest-cuda

Now, you can open http://localhost:8000/health as a smoke-test. You can use http://localhost:8000 as the server configuration inside n8n, however, if you are running n8n in a Docker container you need to use the http://host.docker.internal:8000 server address.




What if I'm running n8n in the cloud?

If you can run the server on the same VPS, you can just use either http://localhost:8000 or http://host.docker.internal:8000 depending on your situation.

If you are running n8n cloud, you should use the address of the VPS - and the right port. Server management is out of the scope if this document.

You can use localtunnel https://theboroer.github.io/localtunnel-www/ to proxy your locally running server to the cloud.



https://drive.google.com/file/d/1ICnK5YSZGZ4uwvA7wdaHBwGrBwbLXrAl/view?usp=sharing

https://drive.google.com/file/d/1JCSnnLTLrdGFHQZMgsjnWDqIidPHSFVg/view?usp=sharing


You don't need fancy tools. You don't need paid APIs. And you definitely don't need to spend a single dollar just to create content because there's a way to generate full long- form videos from script writing to captioning and a speaker image, everything automatically without even paying for anything. And for the system that does exactly that and it builds your entire video from your script, adds captions, places your speaker image, formats the complete layout, and creates the whole thing on its own. It has no limits, no fees, and it does not include any subscription. And since so many of you have been asking for a long form setup, and so we're finally doing it. And for this one, we're going to be using the revenge stories niche because it's one of the fastest growing formats right now. And channels are getting monetized in one month. So creators using setups like this are posting consistently and growing fast, and they're doing it all without spending any dollar. So today I'm going to be showing you exactly how to set this up and start generating the ad revenue from these long form videos effortlessly. So before we start, I just want to let you know that there are two ways to run this setup. The first way is going to be collab server and the second is going to be the local machine. I mean your computer or laptop. So in this video I'm going to be showing you how to run this on a server. The reason is a lot of you guys might not have a good computer or like you know um a good GPU. So that's why we are switching to the cloud and when I say cloud it does not mean that you have to pay uh because Google and the AWS is providing free credits that's almost like you know $300 which is going to last very long. So yeah just stay tuned with the video just follow along and I'm going to show you how to do this for free. Okay the first thing that you guys have to do is just go to this website and then create an account. So once you create an account, you will see a page like this where you can see you will receive up to $200 in credit which is more than enough and it's for for the six months. So we can click on choose free plan. Then you will see a page like this and then you have to click on personal and then you have to fill in all the details. just get on with all the five steps and don't worry, it's not going to take any money from your account because it's free and then you're you're just only completing the account and then you're getting the $100 or maybe like $200 credit for free. So once you do that uh once you're finished with this and then you will be on the main homepage. Okay, I'm uh logged into the account. So once you will be in here you have to just click on these three lines or we can just go ahead and click on this one and then uh you have to just go on and create the compute. Click on compute and then you will see this here um EC2 once you are on this page it will just give you a lot of different things in as well. So here uh if you can see here this is an instance here that's what we are going to create. We need to create an instance. Okay. Now, uh you see here this launch instance. Click on it and then just name the instance. Uh basically we're we're creating the server. So the name I would go for is YouTube uh uh tutorial. Okay. Toot. Okay. That's fine. And you can just go with any name. And then here we need to select the Ubuntu. Just click on it. And then uh it says Amazon machine image. Um click on it. And then we have to choose this one 22.04 LTS HVM SSD volume type because it's under the free tier. Just click on that one and click on confirm changes. And once you do that, uh don't need to change anything in here. And that instant type is going to be you can even go with the 1 GB of memory, but that's going to take a lot of time to generate a video. But I would suggest go for 2 GB or 4 GB. Uh for now, I'm going to be going with the 4 GB. It's going to be a bit faster than the sec uh you know the second one. So that one. And then here it says keep pair login. And uh what you can do is simply click on create new key here and keep the name as anything YouTube uh dut and then everything stays same. Just click on create key here. It's it's going to download a file. Just keep it saved or somewhere on the on your uh computer. And then um nothing to change in here. Just click on these two boxes to check it out. And then this is the storage. I'm going to go with uh I think 30. Yes. And you can go with 20 25 depends on you. The more storage that you're going to be using, the more the you know the charges that they are going to charge. Okay. And then here uh we can simply go ahead and click on launch instance. All right. So it's going to launch the instance. Okay. And then if you can see here this it says EC2 instances. Just click on the instances. You would see that your instance is going to be running. Okay. It says the status is running. We click on it. You can see all of the details. It's running and IP address and everything. So now what you have to do is just simply uh go back to this here. It says security groups. Okay. And there uh in your case you will have this default and you will have this launch wizard one. But but in my case I'm going to be using the second one because I already have one instance. So you will only have this two things uh default and launch wizard one. That's it. So you will click on the launch wizard. Once you have that then you have to click on edit inbound rules. And then here you have to click on add rule custom TCP. And here you have to write 8,000. And here it's going to be this one uh 000000. And then you can just write uh YouTube API. Okay, no need but it's fine. All right. Uh it's done. And then just go back to the instances. Now we have to see if it's running. Okay. We click on this YouTube uh this instance. Right now it says running. Now we have to click on connect. Let's see if it's running. So once we click on connect then it says connect here again. Don't need to change any of things in here. So it's going to open up a new page. Okay. Okay. Uh the instance is running. I can see the terminal here. Now we need to first thing that we need to do is we need to install the Docker. Okay. I'm going to give you this complete uh document which contains all of the things. The first thing that we need to do is install the Docker. We can copy this command and then simply copy it, paste it in here and hit enter. So it's going to install the Docker in this um instance. So let's wait. Okay, it says scanning processes. Uh I think it has installed the Docker. Let me check. Uh since we have the Docker verification, we need to check if it has installed the Docker or not. So let's just go and paste it and run this. So yes, it has installed the Docker. If you can see here, perfectly. And then it says the next command is pull the image. Just copy this command and paste it in here. Paste it. Now we need to pull the docker image which is going to be this. Basically this is going to be the main server that we're going to be installing to kind of generate the videos. You know the main important server. So it's almost like 3.3 GB. It's going to just download the server. Then it's going to start extracting it. So, we're going to have to wait for that. I'm going to just fast forward the video for this one. All right, the image has been completely, you know, pulled and everything. It's done. Now, let's go to the next command, which is going to be running the container. Okay, and paste it. And then, okay, the container is running. Now, let's see if it's running or not. Copy, paste it. Okay, it says uh it has been created at 7 and the status is up. Perfect. It's running now. We can even go with this uh as well to check the logs and everything. Let's go ahead and do that. Okay, it says it's running. But we can see even the live logs if we want to. Uh that means whenever we make make any videos, we can we will be able to see this. Now it's on the live command. That means whenever we do anything with the server, we send in a request, we do something, it's going to come up here. I mean the results. Now um the container is completely running. All of the things are completely done. Now go back to this page. We have this IP address. This is going to be public IP address. This one. Copy this one. Open a new tab. Paste it. Add this 8,000/halth. If it says okay, that means it's perfectly running. See the status says here okay which means it's running perfectly fine. We can close it. Now uh we can use the same address since the server is running and now we have to go to the NA10. If you can see here this is the workflow that we have that we're going to be using to generate all of these videos and post it on YouTube. So basically it's a very simple workflow I've designed. So I'm going to be explaining you about the workflow. But first, as you can see here, it says set me up first because it's the main important thing to set up first. So, if I double click on it, it's just going to show you all of those things because this is important because this is a you know node which contains all of these links like background video URL, the image on the video either it could be a female or the male and the server URL and the language code and the voice. Right? So, this is the thing that we need to set up first and then we can execute the workflow to make a video. But first thing let's um set up all these things first. But to do that uh first we have to make an account in superbase. Uh I already have one. Uh so you just uh go in the superbase. Create an account create a project and then just go to the storage. Uh I'll just show you directly what you see once you log into the superbase for the first time. You will see something like this. You have to click on here. You know it says uh new project. New project. just provide that uh name to the project. Once you're inside the project then you go to the storage and then you click on new bucket. Write any name on the bucket here but make sure it's the public you know it shouldn't be turned on. And then you just going to just go and create this bucket and what it will do it it's going to create this bucket where you can just keep all of the files. So, I already have like sample things in here uh that I just showed you, you know, as a sample videos. But I'm going to do one thing. I'm just going to go ahead and delete all of these things. We can just go ahead and upload all the new things. Just give me a second. Let's delete all of these things. Okay, perfect. Now, let's drag and drop the files. Um since I already have two video like video and two images. What you can do is simply go to pixels and then simply um just write any just click on videos write like sunset any video that you would like to have as a background just go ahead and click on it and then make sure that you download this you know 1080p this full HD click on it download the file that's it and then what you have to do is also um download some pictures that you want to you know showcase you know the front side or any image image it should be without background. Okay, in case if you want to remove the background, you can use um this website to remove the background for free. Okay, and then yeah, that's once you're done with that, just go on to the superbase and drag and drop all those files. I'm just going to simply go ahead and click on upload files and then it's going to open up the file explorer and I'm going to be uploading all of these files. So, this one, this one, and that one. And then that's it. It is going to be uploading all of these files. Once it's done, we're going to be copying the URL. That's the main thing that why why are we doing this? 90%. Okay, I think it's done. 99% and 100% right. Perfect. So, this is going to be the image. And if you can see here, this is going to be the boy. Click on get URL. You can see here. And then go back to the uh it says person mail image URL. Just go here and then paste this inside this. That's perfect. And then go back to the super basease. Click on this another girl image. Click on get URL. This is the girl image that we're going to be using. And then this is a female image. And then paste it in here. And then we need the video. This is going to be the video. Click on get URL. And then same thing MP4. And then that's it. Perfect. Now here we need to set up the server URL. The server URL is going to be the same thing here. This is the public IPv4 address. You just copy this and then paste this instead of you know like this IP address. Rest of it stays the same. Uh I have already uh you know showed this in the document just you know paste it. So the URL is going to be http the this address and then uh colon and then 8,000. That's going to be the port address. Okay. And the language is going to code is going to be the English. English US NG enu. And the voice is going to be M Eric. But in this case, I'm going to be changing this. So in order to see what what is the voice code because you cannot just write anything in here. In order to see what is the voice code, if you go to this document, it says um check language and voices. This is the URL that you have to go on. But in this case, we have to change the IP address. So, I'm going to go ahead and just paste it in here. And our IP is going to be this. And then I'm going to be changing this instead of this one. And paste it. If you can see here, this gives us all of the voices and their, you know, chords and everything. So, you can use any different voices um like, you know, um any girl voices, French voices, you know, English, whatever. So, we're going to be going to the AF hard. I think this one is the good one. Copy that code. Go back to the NA10 and paste it in here. That is it. Okay. So, it says AF, which means it's a female voice. If it says AM, if you can see here, AF, AF, AF, and AM is going to be the male voice. Okay. So, this time we're going to be using the female voice. It's a AF heart. And then it's going to be English. And that's it. Perfect. Now since we have set this up all of the things. So I'm going to be explaining you exactly how this workflow like executes how it works. So basically we have this form. Uh if you can check here we have all of details. Uh so what are the details that we are getting the from this form is story idea. What is the story that you want to create the video on the character name. Okay. So once you do that it's just going to send this to this and but here we are already filled with the details. nothing to do with that. And then here is the story. This is the story where it writes the actual story based on the input that we just gave using the form. So this is the prompt that we have used. Here we are just only giving the name of the character and then here is the content which is going to be the story. The simple idea that we are just providing uh this um in this script in this prompt and then here is a system prompt. It's very simple. You're an expert creative writer. you're uh you write revenge stories for a living. Okay, very simple one. And then when you execute that, it's going to write a complete story based on that information that you provide. Okay, and then we have this cleanup text. Maybe sometimes in the in our story, we have like you know dashes and everything. It removes that. And then after that we're going to be creating a title as well. So in this here uh we have this we are just you know kind of using this story and then using openAI's uh chat model to kind of create a title based on the story. So we we are just providing him all the story details and everything and then it provides us a good title and under 100 characters. It's already written here. Uh the title should be maximum 100 characters. Okay. So once we do that we are also going to clean up the text that mean the title just in case uh to remove the you know dashes and slashes and everything. Okay. And then comes the real part which is going to be the https node where we are actually creating the video. So basically this is uh the you know uh the server URL that we are using. If you can see here this and then it says slash and API videos which is the the end point to create the videos using the server. So this is the main important thing here. You don't need to change anything in here unless you're going to be changing something in that you know setup node. So this is the important thing. Now here as well we have the send body. First thing is going to be the text which is going to be the uh the story itself. So we can just simply go ahead and drag and drop this text in here and it just kind of gives us the complete story to the server. And then we have the person's image URL. And here we it depends on you what type of um do you want to just you know um let's just say a person's image or a girl image or a female image. You can you can just simply go ahead and get that you know URL from here. Simply drag and drop it here directly. And then we need the person's name. Uh we already have that from the form and just simply just correct character name and just drag and drop it here. The background URL is going to be from this node directly. Just drop it in here. And the voice is going to be exactly from this here. Just drop it in here. Perfect. That's simple. That very simple. Then you execute the steps and then it's going to start creating a video. Okay. And then we have the wait node. Check the status node. we have this kind of loop going on until it just gives you the you know video for to download. So then we have the final um node to kind of download the video and then it just um kind of uploads the video on the YouTube as well without you doing anything. Now let's go ahead and execute the workflow and see how this actually generates the video. I'm going to be explaining you everything like uh you know what's happening in behind the scenes and everything. Let's go ahead and execute. So, it's going to pop up the form. Let's just write this wife cheating on her husband with his brother. Let's go with the name uh Daniel is fine, I guess. No, no, let's go with Tyler. Submit. Once we submit, it's going to execute the script and then it's writing the story right now. And then it's going to clean up the text and clear the title. So uh when it's like right now it's working. So I'm going to be explaining one more thing here. So if you can see here uh it has the uh on form submission node here. So we can replace this with the if you kind of want to automate this completely uh you know for YouTube faceless channels. What you can do is you can simply go ahead and replace this with a telegram node and you just send hi message on this telegram and it executes the workflow. So right now it starts creating the workflow and you can even check it out from the back end and you can see here uh it has done something. It is now getting the request and everything. it is in the process even if you can go to this um URL and uh the IP address and then colon 8000 and docs it will give you this this thing and then what you can do is you can simply see here as well the the video status it will say it's in the processing you know you can just create videos delete videos directly from here as well I'm just kind of showing you the back end of all of this it's not important that you have to do with this but it just for those who kind of want to just you know um gets to know everything about the server. Okay. So right now it's still creating the videos and everything is going in here. So we need to wait a little bit. Uh so it's in the loop right now. So it's going to just check um keeps checking after every 10 seconds. So I'm going to be explaining other thing that we can do here. So um I have one other thing that I have done with another workflow which is basically not this. Uh it's a very simple one. Uh the complex one is kind of different. I'm going to show you that. Which one is that? Give me a second. That's a very different one, but I'm going to be showing you that complex one is really great. Um but I would have like um I will I will also give this to you as well, but this would be bit complex. Um basically this thing it does um adds the stories to your database. This is uh the stories that it has added already. It's it just you know scrapes data from Reddit and it just does its own thing and just makes this table and then um after that once this is finished then we execute the workflow. The workflow does the same thing. It just checks up everything and then it checks the database this it sees which story is it's going to create. Uh if the status says skipped it will skip that. If the status says Q then it will just you know start the story with this and after that uh it creates the character and it writes the story. Then it creates title and then it just um you know checks out the voices that we are going to be using and then it's the same thing. Then after that once the video is created it just um loads the database with the status change that it has been created and then it posts the video on YouTube and it also then uploads the status on uh in here that you know the video is uploaded. All right. So this is something that you can do as well. So let's just check the the status. It's still waiting. So we can see it's still in the process. It's going to take a while. I would say like 4 to 5 minutes. All right. So I'm going to go back uh get back to you guys and after it's finished. Okay. So as you can see here it says completed video in this back end. We don't need to see this but I'm just kind of showing you. So right now uh it just going to download the video and then post it on the YouTube. So um until it's just going I'm going to just show you directly uh what the video is about. So we can go to this URL and open it and it will download the video directly. It's right now downloading. So this is a video and it's almost like 5 minutes 6 minutes video. Uh as you can see here this is perfectly fine. We can change the background depends on you. And you might ask me why is this having a space in here. Uh it says because uh when we download this picture it has a you know transparent background uh like this. So basically the image was you know in this format. So yeah that's the reason if we have if we have cropped the image from here and here then this image would have been completely to the left side and it would be like completely fine but that's not an issue you can just uh figure that out not in problem with that so yeah that's it um uh this is a complete good video and you can just simply post it on YouTube and just do this on autopilot and I I think within a month uh if you just go for like four videos a day and within a month um your channel might be monetized because I have seen different YouTube channels. I have built this automation for two of the clients and one of the clients has monetized the channel and one of uh the other client uh which is still you know um posting videos because see it's all depends on what kind of stories that you would like to post. Uh it depends on you. If the story is good yeah I think it's going to go viral. So that's it. So, if you guys like the video, please drop a like and share the video and also comment in the comment section if you have any problems. I will definitely help you guys. And yes, I'm thinking uh I should start the Discord channel because a lot of you guys are having a different problems in running these workflows. So, there's no way to kind of help you uh you know, it's kind of hard for you guys to and for me as well to kind of you know, connect with you over Instagram and then sharing the screenshots. It takes time. So I think we should start uh the discord channel and then we can build a good community over there and then we can help each other. There there might be a lot of people from you know um in the channel that knows very well about these workflows. They can help other people. They can help me. They can give me ideas about what kind of videos I should make. I think that would be much better than um you know just you know um helping you guys over Instagram or something. So what do you think guys? Please comment uh you know in the you know comment section. Yes, we should start discord and then yeah, I think I'll do it. And guys, and thank you so much for giving so much love to the channel and we are very close to 1.5K. And one more thing I would like to know is uh you know just watch the complete video and also watch other stuff as well because our channel is not yet monetized but it will be in couple of months I guess or maybe like in couple of days or depends on you. Yeah. Thank you so much. Have a good day.












































































