# AGENTE 03: ANALISTA
> **Timestamp**: T=2  
> **Responsabilidade**: Analisar v√≠deos coletados, fazer clustering e identificar padr√µes emocionais

---

## üéØ OBJETIVO

Processar CSV bruto, limpar dados, fazer clustering sem√¢ntico de t√≠tulos, e identificar 4-5 grupos emocionais para criar eixos.

---

## üì• INPUT

### Arquivo: `outputs/T01_canais_referencias.csv`

M√≠nimo 200 v√≠deos com campos obrigat√≥rios.

---

## üì§ OUTPUT

### Arquivo: `outputs/T02_clusters.json`

```json
{
  "timestamp": "T=2",
  "estatisticas": {
    "videos_brutos": 342,
    "videos_limpos": 215,
    "clusters_identificados": 4,
    "videos_descartados": 127
  },
  "clusters": [
    {
      "id": "cluster_0",
      "nome": "Emo√ß√£o Identificada",
      "descricao": "Padr√£o emocional dominante",
      "tamanho": 55,
      "exemplos_titulos": [
        "t√≠tulo exemplo 1",
        "t√≠tulo exemplo 2",
        "t√≠tulo exemplo 3"
      ],
      "metricas": {
        "views_medias": 500000,
        "engajamento_medio": 0.08,
        "viral_score_medio": 1.5
      },
      "palavras_chave": ["palavra1", "palavra2", "palavra3"],
      "emocao_central": "injusti√ßa + repara√ß√£o",
      "saturacao": "m√©dia",
      "forca_viral": "alta"
    }
  ]
}
```

---

## üîß PROCESSO DE LIMPEZA

### Etapa 1: Remo√ß√£o de Duplicatas
```python
# Por video_id
df_limpo = df.drop_duplicates(subset=['video_id'])
```

### Etapa 2: Filtros
```python
# Remover:
- M√∫sicas/Clipes (buscar "official music", "lyric video")
- Shorts < 60s (muito curtos)
- V√≠deos > 30min (muito longos para Dark)
- Views < 10.000 (sem tra√ß√£o)
- Canais com < 3 v√≠deos na amostra (inconsistentes)
```

### Etapa 3: Enriquecimento de M√©tricas
```python
# Criar novas colunas
df['VPH'] = views / horas_desde_publicacao
df['engajamento'] = (likes + comentarios) / views
df['viral_score'] = views_video / media_views_canal
```

---

## ü§ñ CLUSTERING (HDBSCAN)

### Prepara√ß√£o
```python
from sentence_transformers import SentenceTransformer
import hdbscan

# 1. Vetorizar t√≠tulos
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
embeddings = model.encode(df['titulo'].tolist())

# 2. Clustering
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=15,  # M√≠nimo 15 v√≠deos por cluster
    min_samples=5,
    metric='euclidean'
)

labels = clusterer.fit_predict(embeddings)
df['cluster'] = labels
```

### An√°lise de Clusters
```python
for cluster_id in set(labels):
    if cluster_id == -1:  # Ru√≠do
        continue
    
    cluster_videos = df[df['cluster'] == cluster_id]
    
    # Extrair padr√µes
    top_palavras = extrair_keywords(cluster_videos['titulo'])
    emocao = identificar_emocao(top_palavras, cluster_videos)
    
    cluster_info = {
        "id": f"cluster_{cluster_id}",
        "tamanho": len(cluster_videos),
        "exemplos_titulos": cluster_videos['titulo'].head(5).tolist(),
        "palavras_chave": top_palavras[:10],
        "emocao_central": emocao,
        ...
    }
```

---

## üé≠ IDENTIFICA√á√ÉO DE EMO√á√ïES

### Padr√µes Emocionais a Buscar
```python
PADROES_EMOCIONAIS = {
    "humilha√ß√£o ‚Üí revanche": [
        "zombar", "humilhar", "se arrepender", "vingan√ßa", "justi√ßa"
    ],
    "segredo ‚Üí revela√ß√£o": [
        "segredo", "descobrir", "verdade", "revelar", "esconder"
    ],
    "medo ‚Üí al√≠vio": [
        "terror", "medo", "susto", "descobrir", "salva√ß√£o"
    ],
    "injusti√ßa ‚Üí repara√ß√£o": [
        "injusto", "cruel", "triste", "final feliz", "justi√ßa"
    ],
    "curiosidade ‚Üí recompensa": [
        "mist√©rio", "incr√≠vel", "surpreendente", "impressionante"
    ]
}

def identificar_emocao(palavras_chave, videos):
    scores = {}
    
    for emocao, keywords in PADROES_EMOCIONAIS.items():
        score = sum(1 for palavra in palavras_chave if palavra in keywords)
        scores[emocao] = score
    
    return max(scores, key=scores.get)
```

---

## ‚úÖ VALIDA√á√ïES

### Input Validation
- ‚úÖ CSV existe e √© v√°lido
- ‚úÖ M√≠nimo 200 v√≠deos
- ‚úÖ Todos campos obrigat√≥rios presentes

### Output Validation
- ‚úÖ Entre 3-6 clusters identificados
- ‚úÖ Cada cluster tem m√≠nimo 15 v√≠deos
- ‚úÖ Ru√≠do (cluster -1) < 30% dos v√≠deos
- ‚úÖ Emo√ß√µes identificadas para todos clusters

---

## ‚ö†Ô∏è TRATAMENTO DE ERROS

| Erro | A√ß√£o |
|------|------|
| Poucos v√≠deos ap√≥s limpeza | Relaxar filtros (ex: aceitar 5.000 views) |
| Clustering gera 1 cluster s√≥ | Ajustar min_cluster_size |
| Muito ru√≠do (>40%) | Ajustar min_samples |
| Erro de encoding | Remover caracteres especiais |

---

## üìã EXEMPLO CONCRETO

### Input (primeiras 5 linhas do CSV)
```csv
video_id,titulo,views,likes,comentarios,duracao_segundos
abc123,Eles me humilharam... mas se arrependeram,500000,25000,1200,240
def456,10 Mist√©rios que Nunca Foram Resolvidos,800000,40000,2000,480
ghi789,A Verdade Sobre Salom√£o e o Ouro,350000,15000,800,360
...
```

### Output (clusters.json - resumido)  
```json
{
  "timestamp": "T=2",
  "estatisticas": {
    "videos_brutos": 342,
    "videos_limpos": 215,
    "clusters_identificados": 4
  },
  "clusters": [
    {
      "id": "cluster_0",
      "nome": "Humilha√ß√£o e Revanche",
      "tamanho": 55,
      "exemplos_titulos": [
        "Eles me humilharam... mas se arrependeram",
        "Zombaram de mim na escola. Hoje sou CEO",
        "Me chamaram de pobre. Se arrependeram"
      ],
      "palavras_chave": ["humilhar", "zombar", "arrepender", "vingan√ßa"],
      "emocao_central": "humilha√ß√£o ‚Üí revanche",
      "metricas": {
        "views_medias": 450000,
        "engajamento_medio": 0.09,
        "viral_score_medio": 1.8
      },
      "saturacao": "m√©dia",
      "forca_viral": "alta"
    },
    {
      "id": "cluster_1",
      "nome": "Mist√©rios N√£o Resolvidos",
      "tamanho": 48,
      "emocao_central": "curiosidade ‚Üí recompensa",
      "saturacao": "alta",
      "forca_viral": "m√©dia"
    }
  ]
}
```

---

## üß™ TESTES

```python
def test_limpeza_dados():
    df_raw = pd.read_csv("test_data.csv")
    df_clean = limpar_dados(df_raw)
    
    # Nenhum short < 60s
    assert all(df_clean['duracao_segundos'] >= 60)
    
    # Nenhuma duplicata
    assert len(df_clean) == df_clean['video_id'].nunique()

def test_clustering():
    df = carregar_dados_teste()
    clusters = fazer_clustering(df)
    
    assert len(clusters) >= 3
    assert len(clusters) <= 6
    assert sum(c['tamanho'] for c in clusters) >= 150
```

---

## üìä M√âTRICAS

- **Tempo esperado**: 5-10 minutos
- **Complexidade**: Alta
- **Depend√™ncias**: pandas, hdbscan, sentence-transformers
- **Tamanho contexto**: ~180 linhas

---

## ‚úÖ CRIT√âRIO DE SUCESSO

1. ‚úÖ 3-6 clusters identificados
2. ‚úÖ Cada cluster com emo√ß√£o clara
3. ‚úÖ JSON v√°lido gerado
4. ‚úÖ Progress atualizado para T=2

---

**Status**: üìù Spec Completa  
**Pronto para**: Implementa√ß√£o
